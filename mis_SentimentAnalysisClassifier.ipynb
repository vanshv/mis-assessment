{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyuRYyYF2PrH"
      },
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBuaZw3owvyI",
        "outputId": "c2579f8c-53e1-4a43-f4b2-c98916a5550c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "!pip install contractions\n",
        "import contractions\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.sparse import hstack\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MChB9bNQt79T",
        "outputId": "21a8731e-beb3-4ca3-a213-046b3e304ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                                text  category\n",
            "0  when modi promised â€œminimum government maximum...      -1.0\n",
            "1  talk all the nonsense and continue all the dra...       0.0\n",
            "2  what did just say vote for modi  welcome bjp t...       1.0\n",
            "3  asking his supporters prefix chowkidar their n...       1.0\n",
            "4  answer who among these the most powerful world...       1.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/mis_assessment/train3.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/mis_assessment/test3.csv\")\n",
        "\n",
        "df_train = df_train.rename(columns = {'Text': 'text'})\n",
        "df_test = df_test.rename(columns = {'Text': 'text'})\n",
        "\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uajGsns2UGG"
      },
      "source": [
        "# Preprocessing and EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrVl9ZCB3_s3"
      },
      "source": [
        "## Nan Values and Duplicate Values Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs-9IGmt3-j0",
        "outputId": "7e6f3ccb-8d22-447d-aff8-75fd9004d017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nan Values before removal 17397, Duplicate Values before removal 8696\n",
            "Nan Values before removal 0, Duplicate Values after removal 0)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Nan Values before removal {df_train.isna().sum().sum()}, Duplicate Values before removal {df_train.duplicated().sum()}\")\n",
        "df_train = df_train.dropna()\n",
        "df_train = df_train.drop_duplicates(subset='text').reset_index(drop=True)\n",
        "print(f\"Nan Values before removal {df_train.isna().sum().sum()}, Duplicate Values after removal {df_train['text'].duplicated().sum()})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "dMdghSffvLZ2",
        "outputId": "03623897-53c0-4645-f8af-6c850c014879"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>68228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>52317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-1.0</th>\n",
              "      <td>33732</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "category\n",
              " 1.0    68228\n",
              " 0.0    52317\n",
              "-1.0    33732\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.value_counts('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rxi95HW2Waj"
      },
      "source": [
        "## URL Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYxWxbH9usEz",
        "outputId": "c734bdb6-158c-48d9-b297-6dcb3411946c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n"
          ]
        }
      ],
      "source": [
        "url_count = 0\n",
        "\n",
        "regex_tokenizer = RegexpTokenizer('/', gaps = True)\n",
        "def join_and_split(words):\n",
        "   return ((\" \").join(words)).split()\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(regex_tokenizer.tokenize)\n",
        "df_train['text'] = df_train['text'].apply(join_and_split)\n",
        "# df_test['text'] = df_test['text'].apply(regex_tokenizer.tokenize)\n",
        "# df_test['text'] = df_test['text'].apply(join_and_split)\n",
        "\n",
        "def get_words_from_urls(words):\n",
        "    global url_count\n",
        "    clean_sentence = \"\"\n",
        "    for word in words:\n",
        "        if word[:4] == 'http' or word[:3] == 'www':\n",
        "          url_count += 1\n",
        "          continue\n",
        "        clean_sentence += word + \" \"\n",
        "    return clean_sentence\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(get_words_from_urls)\n",
        "# df_test['text'] = df_test['text'].apply(get_words_from_urls)\n",
        "print(url_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnY7lMbn2dYN"
      },
      "source": [
        "## Expanding contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-62trW7qaLo"
      },
      "outputs": [],
      "source": [
        "df_train['text'] = df_train['text'].apply(contractions.fix)\n",
        "# df_test['text'] = df_test['text'].apply(contractions.fix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOVhn0z12g1Z"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9wMp5TE4TfC",
        "outputId": "d89cc1c4-8c83-4546-84e1-9daf2acb4fea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# we do not convert the sentences to lowercase, because lowercase and uppercase words might have different connotations\n",
        "def lemmatize_sentence(tokenized_list):\n",
        "    lemmatized_list = [lemmatizer.lemmatize(word) if len(word) > 4 else word for word in tokenized_list]\n",
        "    lemmatized_s = ' '.join([str(word) for word in lemmatized_list])\n",
        "    return lemmatized_s\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(word_tokenize)\n",
        "df_train['text'] = df_train['text'].apply(lemmatize_sentence)\n",
        "\n",
        "# df_test['text'] = df_test['text'].apply(word_tokenize)\n",
        "# df_test['text'] = df_test['text'].apply(lemmatize_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C26rovob2lD5"
      },
      "source": [
        "## Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YasaW_KIZqM"
      },
      "outputs": [],
      "source": [
        "df_train['text'] = df_train['text'].str.strip()\n",
        "df_train['text'] = df_train['text'].apply(lambda x : re.sub('[^a-zA-Z ?!]+', '', x))\n",
        "df_train['text'] = df_train['text'].str.replace(r'\\s+', ' ', regex=True)\n",
        "# df_test['text'] = df_test['text'].apply(lambda x : re.sub('[^a-zA-Z ?!]+', '', x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN5bVXs_vLZ4"
      },
      "source": [
        "## Removing Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvxDmUCUvLZ5"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop_duplicates(subset='text').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1El_rXx2vLZ5"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "XUdf2wD7vLZ5",
        "outputId": "f1f6c506-79fc-47e4-8f6f-0213465ac75a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when modi promised minimum government maximum ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talk all the nonsense and continue all the dra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what did just say vote for modi welcome bjp to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking his supporter prefix chowkidar their na...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer who among these the most powerful world...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0    when modi promised minimum government maximum ...\n",
              "1    talk all the nonsense and continue all the dra...\n",
              "2    what did just say vote for modi welcome bjp to...\n",
              "3    asking his supporter prefix chowkidar their na...\n",
              "4    answer who among these the most powerful world...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_comments = df_train[\"text\"]\n",
        "# test_comments = df_test[\"text\"]\n",
        "\n",
        "all_comments = pd.concat([train_comments])\n",
        "\n",
        "all_comments.shape\n",
        "all_comments.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uQ-kLADB4fn9"
      },
      "outputs": [],
      "source": [
        "mf = 30000\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf = True,\n",
        "                             strip_accents = 'unicode',\n",
        "                             analyzer = 'word',\n",
        "                             token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b\\\\w{,1}',\n",
        "                             lowercase = True,\n",
        "                             stop_words = 'english',\n",
        "                             ngram_range = (1, 3),\n",
        "                             min_df = 2,\n",
        "                             max_df = 0.6,\n",
        "                             norm = 'l2',\n",
        "                             max_features = int((3/5)*mf)\n",
        "                             )\n",
        "vectorizer.fit(all_comments)\n",
        "train_word_features = vectorizer.transform(train_comments)\n",
        "# test_word_features = vectorizer.transform(test_comments)\n",
        "\n",
        "char_vectorizer = TfidfVectorizer (sublinear_tf = True,\n",
        "                                   strip_accents = 'unicode',\n",
        "                                   analyzer = 'char',\n",
        "                                   ngram_range = (2, 6),\n",
        "                                   min_df = 2,\n",
        "                                   max_df = 0.6,\n",
        "                                   max_features = int((2/5)*mf)\n",
        "                                   )\n",
        "char_vectorizer.fit(all_comments)\n",
        "train_char_features = char_vectorizer.transform(train_comments)\n",
        "# test_char_features = char_vectorizer.transform(test_comments)\n",
        "\n",
        "train_features = hstack([train_word_features, train_char_features])\n",
        "# test_features = hstack([test_word_features, test_char_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrY1fEdivLZ6"
      },
      "source": [
        "## Feature Engineering and One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iCPH9Na_vLZ6"
      },
      "outputs": [],
      "source": [
        "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split(\" \")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nVACHO9vLZ6"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A2SoUMZ_vLZ6",
        "outputId": "98ecf26d-2e0f-45a6-a8db-88dff62858f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<121124x30001 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 40775819 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix, hstack\n",
        "\n",
        "word_count_array = df_train['word_count'].to_numpy()\n",
        "word_count_array = word_count_array.reshape(-1, 1)\n",
        "word_count_sparse = csr_matrix(word_count_array)\n",
        "X = hstack([train_features, word_count_sparse])\n",
        "# X = train_features\n",
        "\n",
        "y = df_train['category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=3)\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPvA1C8GvLZ6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKC_jcJEvLZ6"
      },
      "source": [
        "## Ridge Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W_EVctZ_vLZ7",
        "outputId": "f5d2a94c-aa82-418a-f2b5-ba58693d02bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy : 0.9428271853637594\n",
            "Validation Accuracy : 0.8769525444998514\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "clf = RidgeClassifier().fit(X_train, y_train)\n",
        "\n",
        "print(f\"Training Accuracy : {clf.score(X_train, y_train)}\")\n",
        "print(f\"Validation Accuracy : {clf.score(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qUtj7y8fvLZ7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# clf = GaussianNB()\n",
        "# y_pred = clf.fit(X_train.toarray(), y_train)\n",
        "\n",
        "# print(f\"Training Accuracy : {clf.score(X_train.toarray(), y_train)}\")\n",
        "# print(f\"Validation Accuracy : {clf.score(X_test.toarray(), y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-1ah2rO-0a9v"
      },
      "outputs": [],
      "source": [
        "# replace -1 with 2\n",
        "\n",
        "y_train[y_train==-1] = 2\n",
        "y_test[y_test==-1] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGU3tUfXvLZ7"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"multi:softprob\",\n",
        "    \"num_class\": 3,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"device\": \"cuda\",\n",
        "    'eta': 0.24172217809311938,\n",
        "    'max_depth': 13,\n",
        "    'min_child_weight': 1.204125740865512,\n",
        "    'subsample': 0.9658273955957696,\n",
        "    'colsample_bytree': 0.7733759101125599,\n",
        "    'gamma': 1.788039740353529,\n",
        "    'lambda': 1.2227284430176681,\n",
        "    'alpha': 0.1801118342336058,\n",
        "    'n_estimators': 453,\n",
        "    'colsample_bylevel': 0.7108990821059266,\n",
        "    'max_delta_step': 1,\n",
        "    'grow_policy': 'lossguide'\n",
        "}\n",
        "\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X_train, y_train, verbose=False)\n",
        "\n",
        "preds = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test, preds, multi_class=\"ovr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1BcdQKE8DIT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    log_loss,\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "metrics = {}\n",
        "metrics[\"Accuracy\"] = accuracy_score(y_test, y_pred)\n",
        "metrics[\"Precision (Macro)\"] = precision_score(y_test, y_pred, average=\"macro\")\n",
        "metrics[\"Precision (Weighted)\"] = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "metrics[\"Recall (Macro)\"] = recall_score(y_test, y_pred, average=\"macro\")\n",
        "metrics[\"Recall (Weighted)\"] = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "metrics[\"F1 Score (Macro)\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
        "metrics[\"F1 Score (Weighted)\"] = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "metrics[\"ROC-AUC (OVR)\"] = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\")\n",
        "metrics[\"ROC-AUC (OVO)\"] = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovo\")\n",
        "metrics[\"Log Loss\"] = log_loss(y_test, y_pred_proba)\n",
        "metrics[\"Matthews Correlation Coefficient\"] = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Metrics Summary:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J882gRHV5uZH"
      },
      "source": [
        "## Bayes Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHsalUY99Zhy"
      },
      "outputs": [],
      "source": [
        "# !pip install joblib\n",
        "# import joblib\n",
        "# !pip install optuna\n",
        "# import optuna\n",
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# def objective(trial):\n",
        "#     # Define hyperparameter search space\n",
        "#     params = {\n",
        "#         \"objective\": \"multi:softprob\",\n",
        "#         \"num_class\": 3,\n",
        "#         \"tree_method\": \"hist\",\n",
        "#         \"device\": \"cuda\",\n",
        "#         \"learning_rate\": trial.suggest_float(\"eta\", 0.2, 0.5),\n",
        "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
        "#         \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 10),\n",
        "#         \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "#         \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
        "#         \"lambda\": trial.suggest_float(\"lambda\", 1, 5),\n",
        "#         \"alpha\": trial.suggest_float(\"alpha\", 0, 5),\n",
        "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
        "\n",
        "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
        "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
        "#         \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
        "#         \"sampling_method\": trial.suggest_categorical(\"sampling_method\", [\"uniform\", \"gradient_based\"]),\n",
        "#         \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
        "#         }\n",
        "\n",
        "#     # Train model\n",
        "#     model = xgb.XGBClassifier(**params)\n",
        "#     model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
        "\n",
        "#     # Evaluate model\n",
        "#     preds = model.predict_proba(X_test)\n",
        "#     auc = roc_auc_score(y_test, preds, multi_class=\"ovr\")\n",
        "#     return auc\n",
        "\n",
        "# # study = optuna.create_study(direction=\"maximize\")\n",
        "# study = joblib.load(f\"/content/drive/MyDrive/mis_assessment/xgb_optuna_study_batch_auc.pkl\")\n",
        "# def save_study_callback(study, trial):\n",
        "#     joblib.dump(study, f\"/content/drive/MyDrive/mis_assessment/xgb_optuna_study_batch_auc.pkl\")\n",
        "#     print(\"saved study\")\n",
        "# study.optimize(objective, n_trials=80, callbacks=[save_study_callback])\n",
        "\n",
        "# print(\"Best hyperparameters:\", study.best_params)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}